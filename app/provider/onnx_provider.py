import logging
from typing import Dict, Any, Optional
import os
import time
import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer
from app.config import get_settings
from app.provider.model_provider import ModelProvider

logger = logging.getLogger(__name__)

class OnnxModelProvider(ModelProvider):
    """
    ì¤‘ì•™ ONNX ëª¨ë¸ ì œê³µì í´ë˜ìŠ¤
    ëª¨ë“  ë²„ì „ì˜ SLLM ì„œë¹„ìŠ¤ê°€ ê³µìœ í•  ìˆ˜ ìˆëŠ” ONNX ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    ì‹±ê¸€í†¤ íŒ¨í„´ì„ êµ¬í˜„í•˜ë˜ global ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    _instance: Optional['OnnxModelProvider'] = None

    def __new__(cls):
        if cls._instance is None:
            logger.info("OnnxModelProvider ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
            cls._instance = super(OnnxModelProvider, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if not getattr(self, "_initialized", False):
            logger.info("OnnxModelProvider ì´ˆê¸°í™”")
            settings = get_settings()
            self.model_path = settings.ONNX_MODEL_PATH
            self.default_model = settings.DEFAULT_ONNX_MODEL
            self.session = None
            self.model_loaded = False
            self.model_load_time = 0
            self.tokenizer = None

            # ëª¨ë¸ ë¡œë“œ ì‹œë„
            self.load_model()
            self._initialized = True

    def load_model(self) -> bool:
        """ONNX ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not os.path.exists(self.model_path):
            logger.warning(f"ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            logger.warning("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        try:
            logger.info(f"ONNX ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
            start_time = time.time()

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct", trust_remote_code=True)

            # ONNX Runtime ì„¸ì…˜ ì˜µì…˜ ì„¤ì •
            session_options = ort.SessionOptions()
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            session_options.intra_op_num_threads = 4

            # ONNX ëª¨ë¸ ë¡œë“œ
            self.session = ort.InferenceSession(
                self.model_path,
                session_options,
                providers=['CPUExecutionProvider']
            )

            self.model_load_time = time.time() - start_time
            self.model_loaded = True
            logger.info(f"ONNX ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ì†Œìš” ì‹œê°„: {self.model_load_time:.2f}ì´ˆ")

            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            input_names = [input.name for input in self.session.get_inputs()]
            output_names = [output.name for output in self.session.get_outputs()]
            logger.info(f"ëª¨ë¸ ì…ë ¥: {input_names}")
            logger.info(f"ëª¨ë¸ ì¶œë ¥: {output_names}")

            return True
        except Exception as e:
            logger.error(f"ONNX ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            logger.warning("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

    def is_loaded(self) -> bool:
        """ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        return self.model_loaded and self.session is not None

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "model_loaded": self.model_loaded,
            "model_load_time": self.model_load_time,
            "model_path": self.model_path,
            "default_model": self.default_model
        }

    def run_inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ONNX ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            input_data (Dict[str, Any]): ëª¨ë¸ ì…ë ¥ ë°ì´í„°

        Returns:
            Dict[str, Any]: ëª¨ë¸ ì¶œë ¥ ë°ì´í„°
        """
        logger.info(f"onnx run_inference í˜¸ì¶œ")
        if not self.is_loaded():
            logger.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        try:
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            prompt = input_data.get("prompt", "")
            temperature = input_data.get("temperature", 0.7)
            max_tokens = input_data.get("max_tokens", 1000)

            # í† í¬ë‚˜ì´ì €ë¡œ ì…ë ¥ ì²˜ë¦¬
            inputs = self.tokenizer(prompt, return_tensors="np", padding=False)
            input_ids = inputs["input_ids"].astype(np.int64)
            attention_mask = inputs["attention_mask"].astype(np.int64)

            # ì´ˆê¸° ìƒì„± ID ì„¤ì •
            generated_ids = input_ids.copy()

            # ì¢…ë£Œ í† í° ID ê°€ì ¸ì˜¤ê¸°
            eos_token_id = self.tokenizer.eos_token_id or self.tokenizer.convert_tokens_to_ids("<|endoftext|>")

            # ì¶œë ¥ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            output_names = [o.name for o in self.session.get_outputs()]

            logger.info(f"ğŸ§  ìƒì„± ì‹œì‘: ìµœëŒ€ {max_tokens} í† í°")

            # í† í° ìƒì„± ë£¨í”„
            for step in range(max_tokens):
                seq_len = generated_ids.shape[1]

                # position_ids ì§ì ‘ ìƒì„±
                position_ids = np.arange(seq_len)[None, :].astype(np.int64)

                # ONNX ì…ë ¥ êµ¬ì„±
                onnx_inputs = {
                    "input_ids": generated_ids.astype(np.int64),
                    "attention_mask": np.ones_like(generated_ids, dtype=np.int64),
                    "position_ids": position_ids,
                }

                # ì¶”ë¡ 
                outputs = self.session.run(output_names, onnx_inputs)
                logits = outputs[0]  # shape: (1, seq_len, vocab_size)

                # ë§ˆì§€ë§‰ í† í°ì—ì„œ ìµœê³  í™•ë¥  í† í° ì¶”ì¶œ
                next_token_id = int(np.argmax(logits[:, -1, :], axis=-1)[0])

                # ì¢…ë£Œ ì¡°ê±´
                if next_token_id == eos_token_id:
                    logger.info("ğŸ›‘ ì¢…ë£Œ í† í° ìƒì„±ë¨!")
                    break

                # ë‹¤ìŒ í† í° ì¶”ê°€
                next_token_arr = np.array([[next_token_id]])
                generated_ids = np.concatenate([generated_ids, next_token_arr], axis=-1)

                logger.info(f"step: {step}")

            # ê²°ê³¼ ë””ì½”ë”©
            output_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            logger.info(f"ğŸ“˜ ìƒì„±ëœ ë¬¸ì¥: {output_text}")

            return {
                "text": output_text,
                "usage": {
                    "prompt_tokens": len(input_ids[0]),
                    "completion_tokens": len(generated_ids[0]) - len(input_ids[0]),
                    "total_tokens": len(generated_ids[0])
                }
            }
        except Exception as e:
            logger.error(f"ì¶”ë¡  ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    def close(self) -> None:
        """ëª¨ë¸ ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œí•©ë‹ˆë‹¤."""
        if self.session is not None:
            # ONNX ì„¸ì…˜ì€ ëª…ì‹œì ì¸ close ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ ì°¸ì¡°ë¥¼ ì œê±°
            self.session = None
            self.model_loaded = False

def get_onnx_session() -> ort.InferenceSession:
    """
    ONNX ì„¸ì…˜ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    provider = OnnxModelProvider()
    return provider.session if provider.is_loaded() else None

def get_onnx_model_info() -> dict:
    """
    ONNX ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    provider = OnnxModelProvider()
    return provider.get_model_info()
